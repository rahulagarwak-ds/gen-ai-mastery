{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workout: Evaluation\n",
    "\n",
    "## Setup\n",
    "```bash\n",
    "uv add openai sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 1: Basic Response Check 游릭\n",
    "**Task:** Create heuristic response evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(response: str) -> dict:\n",
    "    \"\"\"Quick quality checks on response.\"\"\"\n",
    "    return {\n",
    "        \"length\": ...,\n",
    "        \"word_count\": ...,\n",
    "        \"is_empty\": ...,\n",
    "        \"is_refusal\": ...,  # \"I cannot\", \"I'm unable to\"\n",
    "    }\n",
    "\n",
    "# Test\n",
    "result = evaluate_response(\"I cannot answer that question.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 2: String Similarity 游릭\n",
    "**Task:** Calculate text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def string_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"Return 0-1 similarity score.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "print(string_similarity(\"hello world\", \"hello there\"))\n",
    "print(string_similarity(\"python\", \"python\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 3: Semantic Similarity 游리\n",
    "**Task:** Use embeddings for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def semantic_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"Return cosine similarity of embeddings.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "print(semantic_similarity(\"I love dogs\", \"Dogs are my favorite\"))\n",
    "print(semantic_similarity(\"I love dogs\", \"The weather is nice\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 4: LLM-as-Judge 游리\n",
    "**Task:** Evaluate response with GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def evaluate_with_llm(\n",
    "    query: str,\n",
    "    response: str,\n",
    "    criteria: str = \"relevance and accuracy\"\n",
    ") -> dict:\n",
    "    \"\"\"Return {score: 1-5, reasoning: str}.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "result = evaluate_with_llm(\n",
    "    \"What is Python?\",\n",
    "    \"Python is a snake species.\",\n",
    "    \"accuracy\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 5: Pairwise Comparison 游리\n",
    "**Task:** Compare two responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query: str, response_a: str, response_b: str) -> str:\n",
    "    \"\"\"Return 'A', 'B', or 'tie'.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "winner = compare_responses(\n",
    "    \"What is 2+2?\",\n",
    "    \"The answer is 4.\",\n",
    "    \"2+2 equals 4, which is the sum of two 2s.\"\n",
    ")\n",
    "print(f\"Winner: {winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 6: Retrieval Metrics 游리\n",
    "**Task:** Calculate precision, recall, MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_metrics(\n",
    "    retrieved: list[str],\n",
    "    relevant: list[str],\n",
    "    k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"Calculate retrieval quality metrics.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "retrieved = [\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\"]\n",
    "relevant = [\"doc2\", \"doc5\", \"doc7\"]\n",
    "print(retrieval_metrics(retrieved, relevant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 7: Faithfulness Check 游댮\n",
    "**Task:** Check if response is grounded in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_faithfulness(\n",
    "    response: str,\n",
    "    context: list[str]\n",
    ") -> dict:\n",
    "    \"\"\"Return {score: 0-1, ungrounded_claims: list}.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "context = [\"Python was created by Guido van Rossum in 1991.\"]\n",
    "response = \"Python was created by Guido in 1991. It's the most popular language.\"\n",
    "# \"most popular\" is not in context - should flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 8: Test Case Runner 游댮\n",
    "**Task:** Build evaluation test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    id: str\n",
    "    query: str\n",
    "    expected: str\n",
    "\n",
    "def run_test_suite(\n",
    "    cases: list[TestCase],\n",
    "    pipeline,  # Callable[[str], str]\n",
    "    threshold: float = 0.8\n",
    ") -> dict:\n",
    "    \"\"\"Run tests and return summary.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "cases = [\n",
    "    TestCase(\"q1\", \"2+2?\", \"4\"),\n",
    "    TestCase(\"q2\", \"Capital of France?\", \"Paris\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Self-Check\n",
    "\n",
    "- [ ] Can implement heuristic checks\n",
    "- [ ] Can calculate string and semantic similarity\n",
    "- [ ] Can use LLM-as-judge pattern\n",
    "- [ ] Can evaluate retrieval quality\n",
    "- [ ] Can check response faithfulness\n",
    "- [ ] Can build and run test suites"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
