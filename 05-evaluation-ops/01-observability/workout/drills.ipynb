{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workout: Observability\n",
    "\n",
    "## Setup\n",
    "```bash\n",
    "uv add langfuse openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 1: LLM Call Logger 游릭\n",
    "**Task:** Create a structured LLM call logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class LLMLog:\n",
    "    model: str\n",
    "    prompt: str\n",
    "    response: str\n",
    "    tokens: int\n",
    "    latency_ms: float\n",
    "\n",
    "def log_call(log: LLMLog):\n",
    "    \"\"\"Print log as JSON.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "log_call(LLMLog(\n",
    "    model=\"gpt-4o\",\n",
    "    prompt=\"Hello\",\n",
    "    response=\"Hi!\",\n",
    "    tokens=10,\n",
    "    latency_ms=250\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 2: Timed LLM Call 游릭\n",
    "**Task:** Wrap OpenAI call with timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def timed_chat(messages: list, model: str = \"gpt-4o-mini\") -> tuple[str, float]:\n",
    "    \"\"\"Return (response, latency_ms).\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 3: Cost Calculator 游리\n",
    "**Task:** Calculate cost of LLM calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing per 1M tokens (approximate)\n",
    "PRICING = {\n",
    "    \"gpt-4o\": {\"input\": 5.00, \"output\": 15.00},\n",
    "    \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "    \"claude-3-5-sonnet\": {\"input\": 3.00, \"output\": 15.00},\n",
    "}\n",
    "\n",
    "def calculate_cost(\n",
    "    model: str,\n",
    "    input_tokens: int,\n",
    "    output_tokens: int\n",
    ") -> float:\n",
    "    \"\"\"Return cost in USD.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "cost = calculate_cost(\"gpt-4o\", 1000, 500)\n",
    "print(f\"${cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 4: Simple Tracer 游리\n",
    "**Task:** Create a context manager for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def trace(name: str):\n",
    "    \"\"\"Print span start and end with duration.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Usage\n",
    "# with trace(\"outer\"):\n",
    "#     with trace(\"inner\"):\n",
    "#         time.sleep(0.1)\n",
    "# Should print nested timing info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 5: Request ID Context 游리\n",
    "**Task:** Use contextvars for request tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextvars import ContextVar\n",
    "import uuid\n",
    "\n",
    "request_id: ContextVar[str] = ContextVar(\"request_id\")\n",
    "\n",
    "def set_request_id() -> str:\n",
    "    \"\"\"Generate and set request ID.\"\"\"\n",
    "    pass\n",
    "\n",
    "def get_request_id() -> str:\n",
    "    \"\"\"Get current request ID.\"\"\"\n",
    "    pass\n",
    "\n",
    "def log_with_context(message: str):\n",
    "    \"\"\"Log message with request ID prefix.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 6: Metrics Aggregator 游리\n",
    "**Task:** Collect and summarize metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from statistics import mean, median\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    count: int\n",
    "    latency_avg: float\n",
    "    latency_p50: float\n",
    "    latency_p95: float\n",
    "    error_rate: float\n",
    "\n",
    "def aggregate_metrics(logs: list[dict]) -> Metrics:\n",
    "    \"\"\"Calculate statistics from logs.\"\"\"\n",
    "    pass\n",
    "\n",
    "logs = [\n",
    "    {\"latency_ms\": 100, \"error\": False},\n",
    "    {\"latency_ms\": 200, \"error\": False},\n",
    "    {\"latency_ms\": 150, \"error\": True},\n",
    "    {\"latency_ms\": 180, \"error\": False},\n",
    "]\n",
    "print(aggregate_metrics(logs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 7: Langfuse Decorator 游댮\n",
    "**Task:** Create a decorator that logs to Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "from functools import wraps\n",
    "\n",
    "# langfuse = Langfuse()  # Uses env vars\n",
    "\n",
    "def observe(name: str = None):\n",
    "    \"\"\"Decorator to trace function calls.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Create trace\n",
    "            # Execute function\n",
    "            # Log result\n",
    "            pass\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@observe(\"my_function\")\n",
    "def my_function(x: int) -> int:\n",
    "    return x * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 8: Async Logger 游댮\n",
    "**Task:** Create non-blocking log batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import deque\n",
    "\n",
    "class AsyncLogger:\n",
    "    def __init__(self, batch_size: int = 10):\n",
    "        self.buffer = deque()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def log(self, entry: dict):\n",
    "        \"\"\"Add to buffer, flush if full.\"\"\"\n",
    "        pass\n",
    "\n",
    "    async def _flush(self):\n",
    "        \"\"\"Send batch to backend.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# logger = AsyncLogger(batch_size=5)\n",
    "# for i in range(12):\n",
    "#     logger.log({\"event\": i})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Self-Check\n",
    "\n",
    "- [ ] Can create structured LLM logs\n",
    "- [ ] Can measure and track latency\n",
    "- [ ] Can calculate LLM costs\n",
    "- [ ] Can implement basic tracing\n",
    "- [ ] Can propagate request context\n",
    "- [ ] Can aggregate metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
