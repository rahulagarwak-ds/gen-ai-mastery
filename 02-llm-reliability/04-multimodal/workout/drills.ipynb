{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workout: Multimodal APIs\n",
    "\n",
    "## Setup\n",
    "```bash\n",
    "uv add openai anthropic pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 1: Basic Vision Call 游릭\n",
    "**Task:** Analyze an image from URL with GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Analyze this image: https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/1200px-Python-logo-notext.svg.png\n",
    "# Ask: \"What logo is this and what does it represent?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                # Add text and image_url content\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 2: Base64 Image Encoding 游릭\n",
    "**Task:** Encode a local image and send to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "def encode_image(path: str) -> str:\n",
    "    \"\"\"Encode image to base64 string.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Encode an image and ask \"Describe what you see\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 3: Multiple Images 游리\n",
    "**Task:** Compare two images in a single request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Send two images and ask: \"What are the differences between these?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"...\"},\n",
    "                # Add two image_url items\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 4: Claude Vision 游리\n",
    "**Task:** Use Claude to analyze an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import base64\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "# Note: Claude requires source.type = \"base64\" or \"url\"\n",
    "# and media_type field\n",
    "\n",
    "# Analyze an image with Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 5: Image Optimization 游리\n",
    "**Task:** Resize and compress an image before sending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "def optimize_for_api(\n",
    "    image_path: str,\n",
    "    max_dimension: int = 1024,\n",
    "    quality: int = 85\n",
    ") -> tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Optimize image for API.\n",
    "    Return (base64_string, estimated_tokens).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test with a large image\n",
    "# optimized, tokens = optimize_for_api(\"large_photo.jpg\")\n",
    "# print(f\"Estimated tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 6: Whisper Transcription 游릭\n",
    "**Task:** Transcribe an audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Transcribe an MP3 file\n",
    "# Note: You'll need an actual audio file to test\n",
    "\n",
    "# with open(\"audio.mp3\", \"rb\") as f:\n",
    "#     transcript = client.audio.transcriptions.create(\n",
    "#         model=\"whisper-1\",\n",
    "#         file=f\n",
    "#     )\n",
    "\n",
    "# print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 7: Timestamped Transcription 游리\n",
    "**Task:** Get word-level timestamps from Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Use response_format=\"verbose_json\"\n",
    "# and timestamp_granularities=[\"word\"]\n",
    "\n",
    "# Print each word with its timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 8: Text-to-Speech 游릭\n",
    "**Task:** Generate speech from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Generate speech saying \"Hello, welcome to the AI course!\"\n",
    "# Save to speech.mp3\n",
    "# Try different voices: alloy, echo, fable, onyx, nova, shimmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 9: Document Extraction with Vision 游댮\n",
    "**Task:** Extract structured data from a document image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "import base64\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "class ReceiptData(BaseModel):\n",
    "    store_name: str\n",
    "    date: str\n",
    "    items: list[str]\n",
    "    total: float\n",
    "\n",
    "def extract_receipt(image_path: str) -> ReceiptData:\n",
    "    \"\"\"Extract receipt data from image.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test with a receipt image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 10: Vision + Audio Pipeline 游댮\n",
    "**Task:** Build a pipeline that describes an image and reads it aloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import base64\n",
    "\n",
    "class ImageNarrator:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def describe(self, image_path: str) -> str:\n",
    "        \"\"\"Get detailed description of image.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def narrate(self, description: str, output_path: str):\n",
    "        \"\"\"Convert description to speech.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def process(self, image_path: str, audio_output: str) -> str:\n",
    "        \"\"\"Describe image and create audio narration.\"\"\"\n",
    "        description = self.describe(image_path)\n",
    "        self.narrate(description, audio_output)\n",
    "        return description\n",
    "\n",
    "# Test\n",
    "# narrator = ImageNarrator()\n",
    "# desc = narrator.process(\"photo.jpg\", \"narration.mp3\")\n",
    "# print(f\"Description: {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Self-Check\n",
    "\n",
    "- [ ] Can send images to OpenAI and Claude\n",
    "- [ ] Can encode images as base64\n",
    "- [ ] Can optimize images to reduce tokens\n",
    "- [ ] Can transcribe audio with Whisper\n",
    "- [ ] Can generate speech with TTS\n",
    "- [ ] Can combine vision and audio in pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
