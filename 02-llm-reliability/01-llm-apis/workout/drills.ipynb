{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workout: LLM APIs\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before starting, ensure you have API keys set:\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "export ANTHROPIC_API_KEY=\"sk-ant-...\"\n",
    "export GOOGLE_API_KEY=\"...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 1: Basic OpenAI Call 游릭\n",
    "\n",
    "**Task:** Make a simple chat completion call to GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Make a call asking \"What is the capital of France?\"\n",
    "# Print the response text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 2: Token Counting 游릭\n",
    "\n",
    "**Task:** Count tokens in a message before sending\n",
    "\n",
    "**Expected:** ~40-50 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "text = \"\"\"\n",
    "Python is a high-level, interpreted programming language known for\n",
    "its clear syntax and readability. It was created by Guido van Rossum\n",
    "and first released in 1991.\n",
    "\"\"\"\n",
    "\n",
    "# Count the tokens for gpt-4o model\n",
    "# Print the count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 3: System Prompt 游리\n",
    "\n",
    "**Task:** Create a call with a system prompt that makes the AI respond only in haiku format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Create a system prompt that instructs the AI to\n",
    "# respond ONLY in haiku format (5-7-5 syllables)\n",
    "# Ask about Python programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 4: Temperature Comparison 游리\n",
    "\n",
    "**Task:** Make 3 calls with different temperatures and compare outputs\n",
    "\n",
    "**Expected:** temp=0 gives same output each time, temp=1.5 gives wild variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"Generate a creative name for a coffee shop\"\n",
    "\n",
    "# Make 3 calls with temperature: 0, 0.7, 1.5\n",
    "# Print all outputs and observe differences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 5: Anthropic Call 游릭\n",
    "\n",
    "**Task:** Make a basic Claude API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "# Ask Claude \"Explain what an API is in one sentence\"\n",
    "# Note: system prompt is a separate parameter!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 6: Streaming Response 游리\n",
    "\n",
    "**Task:** Implement streaming output for OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Make a streaming call asking for a short story\n",
    "# Print each chunk as it arrives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 7: Error Handling 游리\n",
    "\n",
    "**Task:** Handle rate limit errors with retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, RateLimitError\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Create a function that:\n",
    "# 1. Makes an API call\n",
    "# 2. Retries 3 times on RateLimitError\n",
    "# 3. Uses exponential backoff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 8: Token Budget 游댮\n",
    "\n",
    "**Task:** Implement a function that truncates messages to fit a token budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def fit_to_budget(\n",
    "    messages: list[dict],\n",
    "    max_tokens: int,\n",
    "    model: str = \"gpt-4o\"\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Keep only the most recent messages that fit within max_tokens.\n",
    "    Always keep the system message if present.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test with a long conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are helpful.\"},\n",
    "    {\"role\": \"user\", \"content\": \"First message \" * 100},\n",
    "    {\"role\": \"assistant\", \"content\": \"First response \" * 100},\n",
    "    {\"role\": \"user\", \"content\": \"Second message\"},\n",
    "]\n",
    "\n",
    "result = fit_to_budget(messages, max_tokens=200)\n",
    "# Should keep system + most recent that fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 9: Provider Abstraction 游댮\n",
    "\n",
    "**Task:** Create a simple abstraction that works with both OpenAI and Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class LLMProvider(ABC):\n",
    "    @abstractmethod\n",
    "    def complete(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "class OpenAIProvider(LLMProvider):\n",
    "    pass\n",
    "\n",
    "class AnthropicProvider(LLMProvider):\n",
    "    pass\n",
    "\n",
    "# Test both providers with same prompt\n",
    "# providers = [OpenAIProvider(), AnthropicProvider()]\n",
    "# for p in providers:\n",
    "#     print(p.complete(\"What is 2+2?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 10: Cost Estimation 游리\n",
    "\n",
    "**Task:** Estimate the cost before making a call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def estimate_cost(\n",
    "    messages: list[dict],\n",
    "    model: str = \"gpt-4o\",\n",
    "    max_output_tokens: int = 500\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Return dict with:\n",
    "    - input_tokens\n",
    "    - estimated_output_tokens\n",
    "    - estimated_cost_usd\n",
    "\n",
    "    Pricing (per 1M tokens):\n",
    "    - gpt-4o: input=$2.50, output=$10.00\n",
    "    - gpt-4o-mini: input=$0.15, output=$0.60\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in detail.\"}\n",
    "]\n",
    "\n",
    "result = estimate_cost(messages, max_output_tokens=1000)\n",
    "# print(f\"Estimated cost: ${result['estimated_cost_usd']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Self-Check\n",
    "\n",
    "- [ ] Can make calls to OpenAI, Anthropic, Google\n",
    "- [ ] Understand the difference between system and user prompts\n",
    "- [ ] Can count tokens and estimate costs\n",
    "- [ ] Can implement retry logic for API errors\n",
    "- [ ] Can stream responses for better UX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
