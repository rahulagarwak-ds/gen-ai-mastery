{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workout: Document Processing\n",
    "\n",
    "## Setup\n",
    "```bash\n",
    "uv add pymupdf tiktoken beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 1: PDF Text Extraction 游릭\n",
    "**Task:** Extract text from a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_pdf_text(path: str) -> str:\n",
    "    \"\"\"Extract all text from PDF.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test (you'll need a sample PDF)\n",
    "# text = extract_pdf_text(\"sample.pdf\")\n",
    "# print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 2: Fixed-Size Chunking 游릭\n",
    "**Task:** Implement basic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_chars(text: str, size: int = 500, overlap: int = 100) -> list[str]:\n",
    "    \"\"\"Chunk text by character count with overlap.\"\"\"\n",
    "    pass\n",
    "\n",
    "text = \"A\" * 1000 + \"B\" * 1000 + \"C\" * 500\n",
    "chunks = chunk_by_chars(text, size=500, overlap=100)\n",
    "print(f\"Chunks: {len(chunks)}\")\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {len(c)} chars, starts with {c[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 3: Recursive Splitting 游리\n",
    "**Task:** Split on natural boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_split(\n",
    "    text: str,\n",
    "    chunk_size: int = 500,\n",
    "    separators: list[str] = None\n",
    ") -> list[str]:\n",
    "    \"\"\"Split on paragraphs, then sentences, then characters.\"\"\"\n",
    "    if separators is None:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    pass\n",
    "\n",
    "text = \"\"\"First paragraph with multiple sentences. This is another sentence.\n",
    "\n",
    "Second paragraph is here. It also has sentences.\n",
    "\n",
    "Third paragraph is the last one.\"\"\"\n",
    "\n",
    "chunks = recursive_split(text, chunk_size=100)\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {c[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 4: Token-Based Chunking 游리\n",
    "**Task:** Chunk by token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def chunk_by_tokens(\n",
    "    text: str,\n",
    "    max_tokens: int = 100,\n",
    "    overlap_tokens: int = 20\n",
    ") -> list[str]:\n",
    "    \"\"\"Chunk by token count.\"\"\"\n",
    "    pass\n",
    "\n",
    "text = \"This is a test. \" * 100\n",
    "chunks = chunk_by_tokens(text, max_tokens=50, overlap_tokens=10)\n",
    "print(f\"Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 5: Metadata Extraction 游릭\n",
    "**Task:** Extract file metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_file_metadata(path: str) -> dict:\n",
    "    \"\"\"Extract basic file metadata.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# meta = extract_file_metadata(\"any_file.txt\")\n",
    "# print(meta)\n",
    "# Should include: filename, extension, size, modified date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 6: HTML to Text 游리\n",
    "**Task:** Clean HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    \"\"\"Convert HTML to clean text.\"\"\"\n",
    "    pass\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head><title>Test</title></head>\n",
    "<body>\n",
    "<h1>Hello</h1>\n",
    "<p>This is a <b>paragraph</b>.</p>\n",
    "<script>alert('removed')</script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(html_to_text(html))\n",
    "# Should print: Hello\\nThis is a paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 7: Chunk with Metadata 游리\n",
    "**Task:** Add metadata to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_metadata(\n",
    "    content: str,\n",
    "    source: str,\n",
    "    chunk_size: int = 500\n",
    ") -> list[dict]:\n",
    "    \"\"\"Return chunks with metadata.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Each chunk should have:\n",
    "# - content\n",
    "# - source\n",
    "# - chunk_index\n",
    "# - total_chunks\n",
    "# - char_start\n",
    "# - char_end\n",
    "\n",
    "chunks = chunk_with_metadata(\"A\" * 1000, \"test.txt\", 300)\n",
    "for c in chunks:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 8: Clean Text 游릭\n",
    "**Task:** Clean text for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_for_embedding(text: str) -> str:\n",
    "    \"\"\"Clean text: remove extra whitespace, special chars.\"\"\"\n",
    "    pass\n",
    "\n",
    "dirty = \"  Hello    World!!!   \\n\\n\\n  Test  \"\n",
    "clean = clean_for_embedding(dirty)\n",
    "print(f\"'{clean}'\")\n",
    "# Should print: 'Hello World!!! Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 9: Document Processor Class 游댮\n",
    "**Task:** Build complete processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    content: str\n",
    "    metadata: dict\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size: int = 500, overlap: int = 100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def load(self, path: str) -> str:\n",
    "        \"\"\"Load document (TXT, MD, PDF).\"\"\"\n",
    "        pass\n",
    "\n",
    "    def process(self, path: str) -> list[Chunk]:\n",
    "        \"\"\"Load, clean, chunk, and add metadata.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# processor = DocumentProcessor(chunk_size=200)\n",
    "# chunks = processor.process(\"sample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 10: Batch Directory Processing 游댮\n",
    "**Task:** Process all documents in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "def process_directory(\n",
    "    dir_path: str,\n",
    "    extensions: list[str] = [\".txt\", \".md\", \".pdf\"],\n",
    "    chunk_size: int = 500\n",
    ") -> Iterator[dict]:\n",
    "    \"\"\"Yield chunks from all documents in directory.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Usage\n",
    "# for chunk in process_directory(\"./docs\"):\n",
    "#     print(f\"{chunk['source']}: {chunk['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Self-Check\n",
    "\n",
    "- [ ] Can extract text from PDF, HTML, Markdown\n",
    "- [ ] Understand different chunking strategies\n",
    "- [ ] Can implement chunking with overlap\n",
    "- [ ] Can extract and attach metadata\n",
    "- [ ] Can build end-to-end processing pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
