{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workout: Advanced RAG\n",
    "\n",
    "## Setup\n",
    "```bash\n",
    "uv add rank-bm25 sentence-transformers cohere openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 1: Query Expansion ðŸŸ¢\n",
    "**Task:** Generate alternative queries using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def expand_query(query: str, n: int = 3) -> list[str]:\n",
    "    \"\"\"Generate n alternative queries.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "queries = expand_query(\"How to improve RAG accuracy?\")\n",
    "print(queries)\n",
    "# Should return 3-4 related queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 2: HyDE Implementation ðŸŸ¡\n",
    "**Task:** Generate hypothetical document for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyde_query(query: str) -> str:\n",
    "    \"\"\"Generate hypothetical answer to embed.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "hypothetical = hyde_query(\"What is HNSW algorithm?\")\n",
    "print(hypothetical)\n",
    "# Should return a passage explaining HNSW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 3: BM25 Sparse Search ðŸŸ¢\n",
    "**Task:** Implement BM25 keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "documents = [\n",
    "    \"Python is a programming language\",\n",
    "    \"JavaScript runs in the browser\",\n",
    "    \"Python is great for machine learning\",\n",
    "    \"Node.js uses JavaScript on server\"\n",
    "]\n",
    "\n",
    "def bm25_search(query: str, documents: list[str], k: int = 3) -> list[str]:\n",
    "    \"\"\"Search using BM25.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "results = bm25_search(\"Python machine learning\", documents)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 4: Hybrid Search ðŸŸ¡\n",
    "**Task:** Combine dense and sparse retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    query_embedding: np.ndarray,\n",
    "    doc_embeddings: np.ndarray,\n",
    "    documents: list[str],\n",
    "    alpha: float = 0.5,  # Dense weight\n",
    "    k: int = 5\n",
    ") -> list[tuple[str, float]]:\n",
    "    \"\"\"Combine dense and sparse search.\"\"\"\n",
    "    pass\n",
    "\n",
    "# alpha=1.0 â†’ Pure dense\n",
    "# alpha=0.0 â†’ Pure sparse\n",
    "# alpha=0.5 â†’ Balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 5: Reciprocal Rank Fusion ðŸŸ¡\n",
    "**Task:** Merge multiple rankings with RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(\n",
    "    rankings: list[list[int]],  # Multiple ranked doc ID lists\n",
    "    k: int = 60\n",
    ") -> list[tuple[int, float]]:\n",
    "    \"\"\"Fuse rankings using RRF formula.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "ranking1 = [1, 3, 5, 2, 4]  # From dense search\n",
    "ranking2 = [3, 1, 2, 5, 4]  # From sparse search\n",
    "fused = reciprocal_rank_fusion([ranking1, ranking2])\n",
    "print(fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 6: Cross-Encoder Reranking ðŸŸ¡\n",
    "**Task:** Rerank results with cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank(\n",
    "    query: str,\n",
    "    documents: list[str],\n",
    "    top_k: int = 5\n",
    ") -> list[tuple[str, float]]:\n",
    "    \"\"\"Rerank documents using cross-encoder.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "docs = [\n",
    "    \"Python is a snake\",\n",
    "    \"Python programming language was created by Guido\",\n",
    "    \"I love cooking\",\n",
    "    \"Python is used for AI and ML\"\n",
    "]\n",
    "reranked = rerank(\"Python programming\", docs, top_k=2)\n",
    "print(reranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 7: Embedding Cache ðŸŸ¢\n",
    "**Task:** Cache embeddings to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "class EmbeddingCache:\n",
    "    def __init__(self, cache_dir: str = \".cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def _hash(self, text: str) -> str:\n",
    "        pass\n",
    "    \n",
    "    def get(self, text: str) -> list[float] | None:\n",
    "        pass\n",
    "    \n",
    "    def set(self, text: str, embedding: list[float]):\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "cache = EmbeddingCache()\n",
    "cache.set(\"hello\", [0.1, 0.2, 0.3])\n",
    "assert cache.get(\"hello\") == [0.1, 0.2, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 8: Semantic Cache ðŸ”´\n",
    "**Task:** Cache based on query similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SemanticCache:\n",
    "    def __init__(self, threshold: float = 0.95, embed_fn=None):\n",
    "        self.threshold = threshold\n",
    "        self.embed_fn = embed_fn\n",
    "        self._cache = {}  # hash -> {embedding, response}\n",
    "    \n",
    "    def get(self, query: str) -> str | None:\n",
    "        \"\"\"Return cached response if similar query exists.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def set(self, query: str, response: str):\n",
    "        \"\"\"Cache query-response pair.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test: Same meaning, different words should hit cache\n",
    "# cache.set(\"What is Python?\", \"Python is a language\")\n",
    "# result = cache.get(\"What's Python?\")  # Should hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 9: Streaming RAG ðŸŸ¡\n",
    "**Task:** Stream response tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "def stream_rag(\n",
    "    query: str,\n",
    "    context: list[str]\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"Stream RAG response token by token.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Usage\n",
    "# for token in stream_rag(\"What is Python?\", [\"Python is...\"]):\n",
    "#     print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Drill 10: Production RAG Pipeline ðŸ”´\n",
    "**Task:** Build complete optimized pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    use_hybrid: bool = True\n",
    "    use_rerank: bool = True\n",
    "    use_cache: bool = True\n",
    "    alpha: float = 0.5\n",
    "\n",
    "class ProductionRAG:\n",
    "    def __init__(self, documents: list[str], config: RAGConfig):\n",
    "        self.config = config\n",
    "        # Initialize components\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        1. Check cache\n",
    "        2. Expand query\n",
    "        3. Hybrid retrieve\n",
    "        4. Rerank\n",
    "        5. Generate\n",
    "        6. Cache and return\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Self-Check\n",
    "\n",
    "- [ ] Can implement query expansion and HyDE\n",
    "- [ ] Can combine dense and sparse search\n",
    "- [ ] Can rerank with cross-encoders\n",
    "- [ ] Can cache embeddings and responses\n",
    "- [ ] Can stream RAG responses\n",
    "- [ ] Understand latency optimization strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
